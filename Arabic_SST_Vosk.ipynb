{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F3Co6hji8Z1y",
        "outputId": "9413dabd-6d89-440f-fac2-da3212435af2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub\n",
            "Successfully installed pydub-0.25.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pydub\n",
        "# pip install vosk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://alphacephei.com/vosk/models/vosk-model-ar-0.22-linto-1.1.0.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HbH2LVep_b9N",
        "outputId": "1bcdbd66-682a-4ae7-f286-5684e348a55e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-08-09 05:05:28--  https://alphacephei.com/vosk/models/vosk-model-ar-0.22-linto-1.1.0.zip\n",
            "Resolving alphacephei.com (alphacephei.com)... 188.40.21.16, 2a01:4f8:13a:279f::2\n",
            "Connecting to alphacephei.com (alphacephei.com)|188.40.21.16|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1377395170 (1.3G) [application/zip]\n",
            "Saving to: ‘vosk-model-ar-0.22-linto-1.1.0.zip’\n",
            "\n",
            "vosk-model-ar-0.22- 100%[===================>]   1.28G  27.8MB/s    in 48s     \n",
            "\n",
            "2024-08-09 05:06:16 (27.5 MB/s) - ‘vosk-model-ar-0.22-linto-1.1.0.zip’ saved [1377395170/1377395170]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# vosk-model-ar-mgb2-0.4.zip"
      ],
      "metadata": {
        "id": "wD_1gfbYAMUi"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/vosk-model-ar-0.22-linto-1.1.0.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4IZWNGraA17N",
        "outputId": "f5b374bd-b2fb-40b9-931f-eec6a02b8ab8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/vosk-model-ar-0.22-linto-1.1.0.zip\n",
            "   creating: vosk-model-ar-0.22-linto-1.1.0/\n",
            "   creating: vosk-model-ar-0.22-linto-1.1.0/ivector/\n",
            "  inflating: vosk-model-ar-0.22-linto-1.1.0/ivector/splice.conf  \n",
            "  inflating: vosk-model-ar-0.22-linto-1.1.0/ivector/global_cmvn.stats  \n",
            "  inflating: vosk-model-ar-0.22-linto-1.1.0/ivector/final.mat  \n",
            " extracting: vosk-model-ar-0.22-linto-1.1.0/ivector/online_cmvn.conf  \n",
            "  inflating: vosk-model-ar-0.22-linto-1.1.0/ivector/final.dubm  \n",
            "  inflating: vosk-model-ar-0.22-linto-1.1.0/ivector/final.ie  \n",
            "   creating: vosk-model-ar-0.22-linto-1.1.0/am/\n",
            " extracting: vosk-model-ar-0.22-linto-1.1.0/am/frame_subsampling_factor  \n",
            "  inflating: vosk-model-ar-0.22-linto-1.1.0/am/final.mdl  \n",
            "  inflating: vosk-model-ar-0.22-linto-1.1.0/am/tree  \n",
            "   creating: vosk-model-ar-0.22-linto-1.1.0/graph/\n",
            "   creating: vosk-model-ar-0.22-linto-1.1.0/graph/phones/\n",
            " extracting: vosk-model-ar-0.22-linto-1.1.0/graph/phones/disambig.txt  \n",
            " extracting: vosk-model-ar-0.22-linto-1.1.0/graph/phones/optional_silence.int  \n",
            "  inflating: vosk-model-ar-0.22-linto-1.1.0/graph/phones/word_boundary.txt  \n",
            " extracting: vosk-model-ar-0.22-linto-1.1.0/graph/phones/optional_silence.txt  \n",
            "  inflating: vosk-model-ar-0.22-linto-1.1.0/graph/phones/align_lexicon.txt  \n",
            "  inflating: vosk-model-ar-0.22-linto-1.1.0/graph/phones/word_boundary.int  \n",
            " extracting: vosk-model-ar-0.22-linto-1.1.0/graph/phones/silence.csl  \n",
            " extracting: vosk-model-ar-0.22-linto-1.1.0/graph/phones/optional_silence.csl  \n",
            " extracting: vosk-model-ar-0.22-linto-1.1.0/graph/phones/disambig.int  \n",
            "  inflating: vosk-model-ar-0.22-linto-1.1.0/graph/phones/align_lexicon.int  \n",
            " extracting: vosk-model-ar-0.22-linto-1.1.0/graph/disambig_tid.int  \n",
            "  inflating: vosk-model-ar-0.22-linto-1.1.0/graph/HCLG.fst  \n",
            "  inflating: vosk-model-ar-0.22-linto-1.1.0/graph/words.txt  \n",
            " extracting: vosk-model-ar-0.22-linto-1.1.0/graph/num_pdfs  \n",
            "  inflating: vosk-model-ar-0.22-linto-1.1.0/graph/phones.txt  \n",
            "   creating: vosk-model-ar-0.22-linto-1.1.0/rescore/\n",
            "  inflating: vosk-model-ar-0.22-linto-1.1.0/rescore/G.fst  \n",
            "  inflating: vosk-model-ar-0.22-linto-1.1.0/rescore/G.carpa  \n",
            "   creating: vosk-model-ar-0.22-linto-1.1.0/conf/\n",
            "  inflating: vosk-model-ar-0.22-linto-1.1.0/conf/model.conf  \n",
            "  inflating: vosk-model-ar-0.22-linto-1.1.0/conf/mfcc.conf  \n",
            "  inflating: vosk-model-ar-0.22-linto-1.1.0/README  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from vosk import Model, KaldiRecognizer\n",
        "import wave\n",
        "import json\n",
        "import os\n",
        "from pydub import AudioSegment\n",
        "\n",
        "import wave\n",
        "import json\n",
        "from vosk import Model, KaldiRecognizer"
      ],
      "metadata": {
        "id": "IWEJ_9bgDAnn"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to the Arabic model directory\n",
        "model_path = \"/content/vosk-model-ar-0.22-linto-1.1.0\"\n",
        "\n",
        "# Load the Vosk model\n",
        "model = Model(model_path)"
      ],
      "metadata": {
        "id": "_NWA11SNB3Ek"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_mp4_to_wav(input_dir, output_dir):\n",
        "    # Create output directory if it doesn't exist\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    # Iterate through all files in the input directory\n",
        "    for filename in os.listdir(input_dir):\n",
        "        if filename.endswith(\".mp4\"):\n",
        "            mp4_path = os.path.join(input_dir, filename)\n",
        "            wav_filename = os.path.splitext(filename)[0] + \".wav\"\n",
        "            wav_path = os.path.join(output_dir, wav_filename)\n",
        "\n",
        "            try:\n",
        "                # Load the MP4 file\n",
        "                audio = AudioSegment.from_file(mp4_path, format=\"mp4\")\n",
        "\n",
        "                # Export as WAV\n",
        "                audio.export(wav_path, format=\"wav\")\n",
        "\n",
        "                print(f\"Converted {filename} to {wav_filename}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {filename}: {str(e)}\")\n",
        "\n",
        "# Example usage\n",
        "input_directory = \"/content/samples\"\n",
        "output_directory = \"/content/inputs\"\n",
        "convert_mp4_to_wav(input_directory, output_directory)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pBHgQW4RiaBa",
        "outputId": "b09f4854-3547-495a-d7f2-4f18c7e02cd7"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converted WhatsApp Audio_2024-08-05_at_7.35.38_AM.mp4 to WhatsApp Audio_2024-08-05_at_7.35.38_AM.wav\n",
            "Converted WhatsApp Audio_2024-08-05_at_7.34.57_AM.mp4 to WhatsApp Audio_2024-08-05_at_7.34.57_AM.wav\n",
            "Converted WhatsApp Audio_2024-08-05_at_7.35.05_AM.mp4 to WhatsApp Audio_2024-08-05_at_7.35.05_AM.wav\n",
            "Converted WhatsApp Audio_2024-08-04_at_4.06.54_PM.mp4 to WhatsApp Audio_2024-08-04_at_4.06.54_PM.wav\n",
            "Converted WhatsApp Audio_2024-08-05_at_7.35.12_AM.mp4 to WhatsApp Audio_2024-08-05_at_7.35.12_AM.wav\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !ffmpeg -i /content/samples/WhatsApp Audio_2024-08-05_at_7.34.57_AM.mp4 -ac 1 -ar 16000 /content/input/output_file_1.wav"
      ],
      "metadata": {
        "id": "IDP_zMf5Dm20"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to your WAV audio file\n",
        "audio_file = \"/content/inputs/WhatsApp Audio_2024-08-05_at_7.34.57_AM.wav\"\n",
        "\n",
        "# Open the audio file\n",
        "wf = wave.open(audio_file, \"rb\")\n",
        "\n",
        "# Check if the audio file has the correct format\n",
        "if wf.getnchannels() != 1 or wf.getsampwidth() != 2 or wf.getcomptype() != \"NONE\":\n",
        "    print(\"Audio file must be WAV format mono PCM.\")\n",
        "    exit(1)\n",
        "\n",
        "# Initialize the recognizer with the sample rate\n",
        "rec = KaldiRecognizer(model, wf.getframerate())\n",
        "\n",
        "# Transcription result variable\n",
        "final_transcript = \"\"\n",
        "\n",
        "# Process the audio file in chunks\n",
        "while True:\n",
        "    data = wf.readframes(4000)\n",
        "    if len(data) == 0:\n",
        "        break\n",
        "    if rec.AcceptWaveform(data):\n",
        "        result = json.loads(rec.Result())\n",
        "        final_transcript += result.get('text', '') + \" \"\n",
        "    else:\n",
        "        partial_result = json.loads(rec.PartialResult())\n",
        "        print(partial_result.get('partial', ''))\n",
        "\n",
        "# Add the last part of the transcription\n",
        "final_result = json.loads(rec.FinalResult())\n",
        "final_transcript += final_result.get('text', '')\n",
        "\n",
        "# Clean up the transcript (optional)\n",
        "final_transcript = final_transcript.strip()\n",
        "\n",
        "# Print the final transcription\n",
        "print(\"Final Transcription:\")\n",
        "print(final_transcript)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "obRcFqJZDT56",
        "outputId": "fb7d79c2-4c41-40ee-b3eb-c3b536f26f2e"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "مكتوب\n",
            "مكتوب\n",
            "مكتوب\n",
            "مكتوب\n",
            "مكتوب\n",
            "مكتوب\n",
            "مكتوب\n",
            "مكتوب\n",
            "مكتوب لخطاب\n",
            "مكتوب لخطاب\n",
            "مكتوب لخطاب\n",
            "مكتوب لخطاب\n",
            "مكتوب لخطاب\n",
            "مكتوب لخطاب\n",
            "مكتوب لخطاب النقل\n",
            "مكتوب لخطاب النقل\n",
            "مكتوب لخطاب النقل\n",
            "مكتوب لخطاب نقل\n",
            "مكتوب لخطاب نقل\n",
            "مكتوب لخطاب نقل\n",
            "مكتوب لخطاب نقل الموظف\n",
            "مكتوب لخطاب نقل الموظف\n",
            "مكتوب لخطاب نقل الموظف\n",
            "مكتوب لخطاب نقل الموظف من\n",
            "مكتوب لخطاب نقل الموظف من\n",
            "مكتوب لخطاب نقل الموظف من\n",
            "مكتوب لخطاب نقل الموظف من\n",
            "مكتوب لخطاب نقل الموظف من\n",
            "مكتوب لخطاب نقل الموظف من\n",
            "مكتوب لخطاب نقل الموظف من قطاع\n",
            "مكتوب لخطاب نقل الموظف من قطاع\n",
            "مكتوب لخطاب نقل الموظف من قطاع\n",
            "مكتوب لخطاب نقل الموظف من قطاع\n",
            "مكتوب لخطاب نقل الموظف من قطاع\n",
            "مكتوب لخطاب نقل الموظف من قطاع القطاع\n",
            "مكتوب لخطاب نقل الموظف من قطاع القطاع\n",
            "مكتوب لخطاب نقل الموظف من قطاع القطاع\n",
            "مكتوب لخطاب نقل الموظف من قطاع القطاع\n",
            "مكتوب لخطاب نقل الموظف من قطاع القطاع\n",
            "Final Transcription:\n",
            "مكتوب لخطاب نقل الموظف من قطاع القطاع\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U openai-whisper"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "dTcVse28tkva",
        "outputId": "7b6a79a4-3231-4b9f-e535-4c6ec85ebe5d"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai-whisper\n",
            "  Downloading openai-whisper-20231117.tar.gz (798 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m798.6/798.6 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: triton<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (2.3.1)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (1.26.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (2.3.1+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (4.66.5)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (10.3.0)\n",
            "Collecting tiktoken (from openai-whisper)\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton<3,>=2.0.0->openai-whisper) (3.15.4)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper) (0.43.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (2024.6.1)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->openai-whisper)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->openai-whisper)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->openai-whisper)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->openai-whisper)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->openai-whisper)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->openai-whisper)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->openai-whisper)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->openai-whisper)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->openai-whisper)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch->openai-whisper)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->openai-whisper)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->openai-whisper)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2024.7.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->openai-whisper) (1.3.0)\n",
            "Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m46.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n",
            "Building wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20231117-py3-none-any.whl size=801359 sha256=f078300b80e13b6e82eb1967b05954b88f9853ba90ff033ec4034d2377fada42\n",
            "  Stored in directory: /root/.cache/pip/wheels/d0/85/e1/9361b4cbea7dd4b7f6702fa4c3afc94877952eeb2b62f45f56\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, tiktoken, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, openai-whisper\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.20 nvidia-nvtx-cu12-12.1.105 openai-whisper-20231117 tiktoken-0.7.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "whisper"
                ]
              },
              "id": "530f86cfab5044c2a1923699b2f8c2e6"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper\n",
        "\n",
        "model = whisper.load_model(\"base\")\n",
        "result = model.transcribe(\"/content/inputs/WhatsApp Audio_2024-08-05_at_7.34.57_AM.wav\")\n",
        "print(result[\"text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AOlkTiDODfMo",
        "outputId": "d34ee801-2bbf-4ce4-f6f7-808286c97e6f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|███████████████████████████████████████| 139M/139M [00:02<00:00, 71.1MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " بكتب لخطاب الناقل موضف من قطاع القطاع\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CgGFE7KdtvJc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}